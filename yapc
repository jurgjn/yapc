#!/usr/bin/env python
"""
An adhoc peak caller for genomic high-throughput sequencing data such as ATAC-seq, 
DNase-seq or ChIP-seq. Specifically written for the purpose of capturing representative 
peaks of characteristic width in a time series data set with two biological replicates per
time point. Briefly, candidate peak locations are defined using concave regions (regions 
with negative smoothed second derivative) from signal averaged across all samples. The 
candidate peaks are then tested for condition-specific statistical significance using IDR.
"""

import argparse
import collections
import csv
import itertools
import logging
import os
import math
import sys

from pprint import pprint

import idr
import numpy as np
import pandas as pd
import pyBigWig

def prepare_second_derivative_kernel(width, times):
    """
    Return composite kernel that calculates the smoothed second derivative via a single convolution with the input signal.

    Weights of the second derivative kernel: https://doi.org/10.1090/S0025-5718-1988-0935077-0
    Rationale for smoothing 3 times (by default): https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html#Smoothing
    """
    kernel = [-1.0/560, 8.0/315, -1.0/5, 8.0/5, -205.0/72, 8.0/5, -1.0/5, 8.0/315, -1.0/560]
    rolling_mean_kernel = np.ones(width)/float(width)
    for i in range(times):
        kernel = np.convolve(kernel, rolling_mean_kernel)
    ADHOC_LARGE_NUMBER = 1e6 # scale to have better visualisation of d2smooth in IGV
    return ADHOC_LARGE_NUMBER*kernel[::-1]

def is_bw(fp):
    with pyBigWig.open(fp) as fh:
        f = fh.isBigWig()
    return f

def coverage(l_fp_inp, fp_out):
    print('calculating mean of all input signal:')
    l_fh_inp = [pyBigWig.open(fp_inp) for fp_inp in l_fp_inp]
    assert pyBigWig.numpy == 1
    assert all([fh_inp.isBigWig() for fh_inp in l_fh_inp])

    fh_out = pyBigWig.open(fp_out, 'w')
    fh_out.addHeader([(chrom, size) for (chrom, size) in l_fh_inp[0].chroms().items()])

    for chrom, size in itertools.islice(l_fh_inp[0].chroms().items(), None):
        l_val = []
        print(chrom, size)
        for fh_inp in l_fh_inp:
            #val = fill_gap(fh_inp.values(chrom, 0, size, numpy=True)) # This is doing something else.
            val = fh_inp.values(chrom, 0, size, numpy=True)
            #print(val.shape)
            l_val.append(val)
        fh_out.addEntries(chrom, 0, values=np.mean(l_val, axis=0), span=1, step=1)

    # Close files
    for fh_inp in l_fh_inp:
        fh_inp.close()
    fh_out.close()

def d2smooth(fp_inp, fp_out, kernel):
    fh_inp = pyBigWig.open(fp_inp) 
    fh_out = pyBigWig.open(fp_out, 'w')
    fh_out.addHeader([(chrom, size) for (chrom, size) in fh_inp.chroms().items()])

    for chrom, size in itertools.islice(fh_inp.chroms().items(), None):
        print(chrom, size)
        val = fh_inp.values(chrom, 0, size, numpy=True)
        val_d2 = np.convolve(val, kernel, mode='same')
        fh_out.addEntries(chrom, 0, values=val_d2, span=1, step=1)

    fh_inp.close()
    fh_out.close()

def write_gffbed(fp,
                 chrom, start, end, name='', attr=None, score=0, strand='.', 
                 thickStart=None, thickEnd=None, itemRgb='#0072b2', 
                 trackline='#track gffTags=on', v=False):
    df = pd.DataFrame()
    def force_iter(l_):
        try:
            l = list(l_)
            if (len(l) > 1) and not(type(l_) is str):
                return l
            else:
                return list(itertools.repeat(l_, len(df)))
        except TypeError:
            return list(itertools.repeat(l_, len(df)))
    #return list(l) if hasattr(l, '__iter__') else list(itertools.repeat(l, len(df)))
    df['chrom'] = list(chrom)
    df['start'] = force_iter(start)
    df['end'] = force_iter(end)
    def pack_row(ir): return (";".join([("%s=%s" % (k, v)).replace(" ", "%20") for k, v in zip(ir[1].index, ir[1])]))
    attr_ = pd.concat([pd.DataFrame({'Name': force_iter(name)}), attr], axis=1)
    df['name'] = list(map(pack_row, attr_.iterrows()))
    df['score'] = force_iter(score)
    df['strand'] = force_iter(strand)

    if not(thickStart is None):
        df['thickStart'] = force_iter(thickStart)       
    else:
        df['thickStart'] = df['start'].copy().tolist()

    if not(thickEnd is None):
        df['thickEnd'] = force_iter(thickEnd)
    else:
        df['thickEnd'] = df['end'].copy().tolist()

    df['itemRgb'] = force_iter(itemRgb)
    with open(fp, 'w') as fh:
        print(trackline, file=fh)
        df.sort_values(['chrom', 'start', 'end']).to_csv(fh, sep='\t', index=False, header=False, quoting=csv.QUOTE_NONE)
    if v: return df

# https://doi.org/10.1101/gr.153668.112
# For analyses where a single TSS position was required, we considered the distribution of cap 5' ends 
# within the TIC, and selected the position with the most tags (the mode). In the case of a tie (two or more 
# positions with the same number of tags), we selected the mode closest to the median of the TIC.
def nanargmax_median(a):
    a_max = np.nanmax(a)
    a_max_indices = np.flatnonzero(a == a_max)
    if len(a_max_indices) == 0:
        return 0
    return a_max_indices[len(a_max_indices) // 2]

assert nanargmax_median([1,2,3,3,3,2,1]) == 3
assert nanargmax_median([1,2,2,1]) == 2
assert nanargmax_median([1]) == 0

def find_concave_regions_chrom(d2y, chrom='.', tol=1e-10):
    s = np.where(np.diff((d2y < tol).astype(int))==1)[0] + 1
    e = np.where(np.diff((d2y < tol).astype(int))==-1)[0] + 1
    if s[0] > e[0]:
        s = np.insert(s, 0, 0)
    if s[-1] > e[-1]:
        e = np.insert(e, len(e), len(d2y))
    v = [-np.mean(d2y[i:j]) for i,j in zip(s,e)]
    m = [i + nanargmax_median(-d2y[i:j]) for i,j in zip(s,e)]

    df_regions = pd.DataFrame(collections.OrderedDict([
        ('chrom', [chrom]*len(s)),
        ('concave_start', s),
        ('concave_end', e),
        #('val', v),
        ('mode', m),
    ]))

    print('%d raw concave regions on chrom %s' % (len(df_regions), chrom))
    return df_regions

def find_concave_regions(fp_inp, fp_out_tsv, fp_out_bed):
    fh_inp = pyBigWig.open(fp_inp) 
    df_regions = pd.concat([
        find_concave_regions_chrom(d2y=fh_inp.values(chrom, 0, size, numpy=True), chrom=chrom)
        for chrom, size in itertools.islice(fh_inp.chroms().items(), None)], axis=0, ignore_index=True)
    fh_inp.close()
    print('%d peaks total' % (len(df_regions),))
    return df_regions

def scores(fp_inp, chroms, starts, ends, kernel):
    """
    Calculate scores for a list of regions, and return an iterator of the scores.

    chroms, starts, ends -- lists of genomic coordinates specifying the regions
    fp_inp -- name of the BigWig file containing the raw coverage signal
    kernel -- specifies the kernel to convolve the raw signal with (this should match the kernel used to define the concave regions)

    Scores are currently defined as the reverse of the mean smoothed second derivative within the region.
    """
    fh_inp = pyBigWig.open(fp_inp)
    d_chrom_d2smooth = {}
    for chrom, size in itertools.islice(fh_inp.chroms().items(), None):
        val = fh_inp.values(chrom, 0, size, numpy=True)
        val_d2 = np.convolve(val, kernel, mode='same')
        d_chrom_d2smooth[chrom] = val_d2

    for chrom, start, end in zip(chroms, starts, ends):
        yield -np.mean(d_chrom_d2smooth[chrom][start:end])

def run_idr(chroms, starts, ends, scores_rep1, scores_rep2, prefix_out, condition):
    fp_rep1 = prefix_out + '_peaksidr_%(condition)s_rep1.bed' % locals()
    fp_rep2 = prefix_out + '_peaksidr_%(condition)s_rep2.bed' % locals()
    fp_iout = prefix_out + '_peaksidr_%(condition)s.bed' % locals()

    df_ = pd.DataFrame()
    df_['chrom'] = chroms
    df_['start'] = starts
    df_['end'] = ends
    df_['name'] = '.'

    # https://github.com/nboley/idr/blob/74665e73bffb689a440948640c386b1188eea1e3/idr/idr.py#L287-L299
    # https://github.com/nboley/idr/blob/74665e73bffb689a440948640c386b1188eea1e3/idr/idr.py#L845
    # https://github.com/nboley/idr/blob/74665e73bffb689a440948640c386b1188eea1e3/idr/idr.py#L858-L865
    df_['score_rep1'] = pd.Series(scores_rep1).rank()
    df_['score_rep2'] = pd.Series(scores_rep2).rank()
    df_['strand'] = '.'

    # https://github.com/nboley/idr/blob/74665e73bffb689a440948640c386b1188eea1e3/idr/idr.py#L64
    # values don't seem to be used, but seem to be required to be present in the input...
    df_['col6'] = 0 # signalValue?
    df_['col7'] = 0 # pValue?
    df_['col8'] = 0 # qValue?

    df_[['chrom', 'start', 'end', 'name', 'score_rep1', 'strand', 'col6', 'col7', 'col8']].to_csv(fp_rep1, header=False, index=False, sep='\t')
    df_[['chrom', 'start', 'end', 'name', 'score_rep2', 'strand', 'col6', 'col7', 'col8']].to_csv(fp_rep2, header=False, index=False, sep='\t')

    idr_args = '--input-file-type bed --rank score --peak-merge-method max --plot'
    idr_cmd = 'idr %(idr_args)s --samples %(fp_rep1)s %(fp_rep2)s --output-file %(fp_iout)s' % locals()
    print('Running idr on condition %(condition)s:' % locals())
    print(idr_cmd)
    os.system(idr_cmd)

def check_recycle(label, fp, recycle):
    # File does not exist
    if not os.path.isfile(fp):
        logging.info('[compute]\t%s: %s' % (label, fp))
        return True

    # File exists and recycle enabled
    elif recycle:
        logging.info('[recycle]\t%s: %s' % (label, fp))
        return False

    # File exists and recycle not enabled
    else:
        os.remove(fp) # delete existing file
        logging.info('[recompute]\t%s: %s' % (label, fp))
        return True

def main(df_samples, prefix_out, kernel, min_concave_region_width, truncate_idr_input, fixed_peak_halfwidth, recycle):
    # fp_coverage
    fp_coverage = prefix_out + '_coverage.bw'
    if check_recycle('pooled coverage track', fp_coverage, recycle):
        coverage(df_samples['fp'].tolist(), fp_coverage)

    # fp_d2smooth
    fp_d2smooth = prefix_out + '_d2smooth.bw'
    if check_recycle('d2smooth track', fp_d2smooth, recycle):
        d2smooth(fp_coverage, fp_d2smooth, kernel)

    # fp_peaksall -- all raw peaks, scored by D2
    fp_peaksall_tsv = prefix_out + '_peaksall.tsv'
    fp_peaksall_bed = prefix_out + '_peaksall.bed'
    if check_recycle('raw concave regions', fp_peaksall_tsv, recycle):
        # call concave regions
        df_regions = find_concave_regions(fp_d2smooth, fp_peaksall_tsv, fp_peaksall_bed)

        # score concave regions
        for i, r in itertools.islice(df_samples.iterrows(), None):
            fp_ = r['fp']
            col_ = r['sample'] + '_score'
            print('scoring peaks for sample: %s' % (r['sample'],))
            df_regions[col_] = [*scores(fp_, df_regions.chrom.tolist(), df_regions.concave_start.tolist(), df_regions.concave_end.tolist(), kernel)]

        # write all concave regions to output
        df_regions.to_csv(fp_peaksall_tsv, header=True, index=False, sep='\t')
        write_gffbed(fp_peaksall_bed,
            chrom = df_regions['chrom'],
            start = df_regions['concave_start'],
            end = df_regions['concave_end'],
            attr = df_regions[[sample + '_score' for sample in df_samples['sample']]],
            thickStart = df_regions['mode'],
            thickEnd = df_regions['mode'] + 1,
        )

    df_regions = pd.read_csv(fp_peaksall_tsv, sep='\t')
    logging.info('%d raw concave regions found' % (len(df_regions),))

    # discard narrow peaks
    logging.info('%d raw peaks' % (len(df_regions),))
    df_regions = df_regions.query('(concave_end - concave_start) >= @min_concave_region_width').reset_index(drop=True)
    logging.info('%d peaks after discarding narrow concave regions (min_concave_region_width=%d)' % (len(df_regions), min_concave_region_width))

    # rank scores & truncate peak list based on "best" rank => 100,000 peaks
    df_regions['min_rank'] = df_regions[[sample + '_score' for sample in df_samples['sample']]].rank(axis=0, ascending=False).min(axis=1)
    df_regions_idr = df_regions.sort_values('min_rank').head(truncate_idr_input).sort_values(['chrom', 'concave_start', 'concave_end']).reset_index(drop=True)
    logging.info('%d peaks after filtering based on best rank across all conditions (truncate_idr_input=%d)' % (len(df_regions_idr), truncate_idr_input))
    
    col_ = [sample + '_score' for sample in df_samples['sample']]
    str_ = df_regions[col_].head().to_string(line_width=1000, index=False)
    logging.info('curvature index-scores:\n%(str_)s' % locals())

    # run IDR using scores from each condition
    for condition in df_samples['condition'].unique():
        fp_idr = prefix_out + '_peaksidr_%(condition)s.bed' % locals()
        if check_recycle('IDR on %s' % (condition,), fp_idr, recycle):
            run_idr(df_regions_idr['chrom'], df_regions_idr['concave_start'], df_regions_idr['concave_end'], df_regions_idr['%(condition)s_rep1_score' % locals()], df_regions_idr['%(condition)s_rep2_score' % locals()], prefix_out, condition)

        col_prefix = condition
        names_ = [
            'chrom', 'concave_start', 'concave_end', 'name',
            '%(col_prefix)s_scaledIDR' % locals(), 'strand',
            '%(col_prefix)s_localIDR' % locals(), '%(col_prefix)s_globalIDR' % locals(),
            '%(col_prefix)s_rep1_start' % locals(), '%(col_prefix)s_rep1_end' % locals(), '%(col_prefix)s_rep1_score' % locals(),
            '%(col_prefix)s_rep2_start' % locals(), '%(col_prefix)s_rep2_end' % locals(), '%(col_prefix)s_rep2_score' % locals(),
        ]
        df_ = pd.read_csv(fp_idr, names=names_, sep='\t').sort_values(['chrom', 'concave_start', 'concave_end']).reset_index(drop=True)
        df_regions_idr['%(col_prefix)s_globalIDR' % locals()] = df_['%(col_prefix)s_globalIDR' % locals()]

    logging.info('globalIDR:')
    col_ = ['%(col_prefix)s_globalIDR' % locals() for col_prefix in df_samples['condition'].unique()]
    logging.info(df_regions_idr[col_].head().to_string(line_width=1000, index=False))

    # Final peak boundaries -- fixed at flank_len bases either side of the mode
    if fixed_peak_halfwidth is None:
        df_regions_idr['start'] = df_regions_idr['concave_start']
        df_regions_idr['end'] = df_regions_idr['concave_end']
    
    else:
        df_regions_idr['start'] = df_regions_idr['mode'] - fixed_peak_halfwidth
        df_regions_idr['end'] = df_regions_idr['mode'] + fixed_peak_halfwidth + 1

    # Write final output of all peaks
    fp_regions_idr = prefix_out + '.tsv'
    df_regions_idr.to_csv(fp_regions_idr, header=True, index=False, sep='\t')
    logging.info('Wrote all %d IDR-scored peaks to %s' % (len(df_regions_idr), fp_regions_idr))

    # Write final .bed-files at a range of thresholds
    col_ = ['%(col_prefix)s_globalIDR' % locals() for col_prefix in df_samples['condition'].unique()]
    df_regions_idr['max_globalIDR'] = df_regions_idr[col_].max(axis=1)
    for th in [0.001, 0.005, 0.01, 0.05]:
        th_globalIDR = -math.log(th, 10)
        fp_regions_idr_th = prefix_out + '_%(th)s.bed' % locals()
        df_regions_idr_th = df_regions_idr.query('max_globalIDR >= @th_globalIDR').reset_index(drop=True)
        write_gffbed(fp_regions_idr_th,
            chrom = df_regions_idr_th['chrom'],
            start = df_regions_idr_th['start'],
            end = df_regions_idr_th['end'],
            attr = df_regions_idr_th[col_],
        )
        logging.info('Wrote %d peaks at IDR=%.3f to %s' % (len(df_regions_idr_th), th, fp_regions_idr_th))

def parse_input_files(l_inp):
    l_records = []
    l_conditions = l_inp[0::3]
    l_fp_rep1 = l_inp[1::3]
    l_fp_rep2 = l_inp[2::3]
    for (condition, fp_rep1, fp_rep2) in zip(l_conditions, l_fp_rep1, l_fp_rep2):
        l_records.append([condition, condition + '_rep1', fp_rep1])
        l_records.append([condition, condition + '_rep2', fp_rep2])
    df_samples = pd.DataFrame.from_records(l_records, columns=['condition', 'sample', 'fp'])
    df_samples['is_bw'] = df_samples['fp'].map(is_bw)
    assert df_samples['is_bw'].all(), 'All input files do not look like BigWigs...'
    return df_samples

def makedirsp(fp):
    try:
        os.makedirs(fp)
    except:
        if not(os.path.isdir(fp)):
            raise

if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('OUTPUT_PREFIX', help='Prefix to use for all output files')
        parser.add_argument('CONDITION_REP1_REP2', nargs='*', help='Name of the condition, BigWig files of first and second replicates; all separated by spaces.')
        parser.add_argument('--smoothing-window-width', help='Width of the smoothing window used for the second derivative track. If the peak calls aren\'t capturing the peak shape well, try setting this to different values ranging from 75 to 200.', type=int, default=150)
        parser.add_argument('--smoothing-times', help='Number of times smoothing is applied to the second derivative.', type=int, default=3)
        parser.add_argument('--min-concave-region-width', help='Discard concave regions smaller than the threshold specified.', type=int, default=75)
        parser.add_argument('--truncate-idr-input', help='Truncate IDR input to the number of peaks specified.', type=int, default=100000)
        parser.add_argument('--fixed-peak-halfwidth', help='Set final peak coordinates to the specified number of base pairs on either side of the concave region mode.', type=int)
        parser.add_argument('--recycle', help='Do not recompute (intermediate) output files if a file with the expected name is already present. Enabling this can lead to funky behaviour e.g. in the case of a previously interrupted run.', action='store_true')
        args = parser.parse_args()

        logging.basicConfig(
            level=logging.DEBUG, 
            format='%(asctime)s | %(message)s',
            datefmt='%y-%m-%d %H:%M:%S')
        logging.debug('numpy version: %s' % (np.__version__,))
        logging.debug('pandas version: %s' % (pd.__version__,))
        logging.debug('pyBigWig.numpy flag: %s' % (pyBigWig.numpy))
        logging.debug('IDR version: %s' % (idr.__version__))

        df_samples = parse_input_files(args.CONDITION_REP1_REP2)
        s_ = df_samples.to_string(line_width=1000, index=False)
        logging.info('Input samples:\n%(s_)s\n' % locals())

        logging.info('Output prefix: %s' % (args.OUTPUT_PREFIX,))

        # Make output directory + intermediate directories if necessary -- fails sometimes
        #prefix_out = sys.argv[-1]
        #makedirsp(os.path.split(prefix_out)[0])

        main(
            df_samples=df_samples,
            prefix_out=args.OUTPUT_PREFIX,
            kernel=prepare_second_derivative_kernel(args.smoothing_window_width, args.smoothing_times),
            min_concave_region_width=args.min_concave_region_width,
            fixed_peak_halfwidth=args.fixed_peak_halfwidth,
            truncate_idr_input=args.truncate_idr_input,
            recycle=args.recycle,
        )

    except KeyboardInterrupt:
        #logging.warning('Interrupted')
        sys.exit(1)
