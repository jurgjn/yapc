#!/usr/bin/env python
"""
An adhoc peak caller for genomic high-throughput sequencing data such as ATAC-seq,
DNase-seq or ChIP-seq. Specifically written for the purpose of capturing representative
peaks of characteristic width in a time series data set with two biological replicates per
time point. Briefly, candidate peak locations are defined using concave regions (regions
with negative smoothed second derivative) from signal averaged across all samples. The
candidate peaks are then tested for condition-specific statistical significance using IDR.
"""

import argparse
import collections
import csv
import itertools
import logging
import os
import math
import sys

from pprint import pprint

import numpy as np
import pandas as pd
import pyBigWig

import yapc
import yapc.peak_calling
import yapc.idr
import yapc.utils

def parse_input_files(l_inp, pseudoreplicates):
    l_records = []
    if pseudoreplicates:
        l_conditions = l_inp[0::5]
        l_fp_rep1 = l_inp[1::5]
        l_fp_rep2 = l_inp[2::5]
        l_fp_prp1 = l_inp[3::5]
        l_fp_prp2 = l_inp[4::5]
        for (condition, fp_rep1, fp_rep2, fp_prp1, fp_prp2) in zip(l_conditions, l_fp_rep1, l_fp_rep2, l_fp_prp1, l_fp_prp2):
            l_records.append([condition, condition + '_rep1', fp_rep1, False])
            l_records.append([condition, condition + '_rep2', fp_rep2, False])
            l_records.append([condition, condition + '_prp1', fp_prp1, True])
            l_records.append([condition, condition + '_prp2', fp_prp2, True])

    else:
        l_conditions = l_inp[0::3]
        l_fp_rep1 = l_inp[1::3]
        l_fp_rep2 = l_inp[2::3]
        for (condition, fp_rep1, fp_rep2) in zip(l_conditions, l_fp_rep1, l_fp_rep2):
            l_records.append([condition, condition + '_rep1', fp_rep1, False])
            l_records.append([condition, condition + '_rep2', fp_rep2, False])

    df_samples = pd.DataFrame.from_records(l_records, columns=['condition', 'sample', 'fp', 'pseudoreplicate'])
    df_samples['is_bw'] = df_samples['fp'].map(yapc.utils.is_bw)
    assert df_samples['is_bw'].all(), 'All input files do not look like BigWigs...'
    return df_samples

def parse_input_table(path, condition_col, replicate_col, track_col):
    df_samples = pd.read_table(path)[[condition_col, replicate_col, track_col]]
    df_samples.columns = ['condition', 'sample', 'fp']
    df_samples['is_bw'] = df_samples['fp'].map(yapc.utils.is_bw)

    if not all (df_samples['is_bw']):
        df_samples['fp'] = df_samples['fp'].map(lambda fp: os.path.normpath(os.path.join(os.path.dirname(path), fp)))
        df_samples['is_bw'] = df_samples['fp'].map(yapc.utils.is_bw)
    print(df_samples)

    df_samples['pseudoreplicate'] = False
    return df_samples

if __name__ == '__main__':
    try:
        print(f'yapc (yet another peak caller) {yapc.__version__}\n')

        parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('OUTPUT_PREFIX', help='Prefix to use for all output files')
        parser.add_argument('CONDITION_REP1_REP2', nargs='*', help='Name of the condition, BigWig files of first and second replicates; all separated by spaces. Alternatively, location of the sample sheet file')
        parser.add_argument('--condition-col', help='Sample sheet column containing condition identifiers', default='condition')
        parser.add_argument('--replicate-col', help='Sample sheet column containing replicate identifiers', default='sample')
        parser.add_argument('--track-col', help='Sample sheet column with track (BigWig) file locations (relative to either working directory or directory containing the sample sheet)', default='fp')
        parser.add_argument('--smoothing-window-width', help='Width of the smoothing window used for the second derivative track. If the peak calls aren\'t capturing the peak shape well, try setting this to different values ranging from 75 to 200.', type=int, default=150)
        parser.add_argument('--smoothing-times', help='Number of times smoothing is applied to the second derivative.', type=int, default=3)
        parser.add_argument('--min-concave-region-width', help='Discard concave regions smaller than the threshold specified.', type=int, default=75)
        parser.add_argument('--truncate-idr-input', help='Truncate IDR input to the number of peaks specified.', type=int, default=100000)
        parser.add_argument('--fixed-peak-halfwidth', help='Set final peak coordinates to the specified number of base pairs on either side of the concave region mode.', type=int)
        parser.add_argument('--pseudoreplicates', help='Use pseudoreplicates as implemented in modENCODE (Landt et al 2012; around Fig 7): for each condition, assess peak reproducibility in replicates and pseudoreplicates; report globalIDRs for the set with a larger number of peak calls (at IDR=0.001). Pseudoreplicates are specified as the 3rd and 4th file name after every condition.', action='store_true')
        parser.add_argument('--chroms-subset', help='Call peaks only on chroms specified for e.g. quick tests')
        parser.add_argument('--recycle', help='Do not recompute (intermediate) output files if a file with the expected name is already present. Enabling this can lead to funky behaviour e.g. in the case of a previously interrupted run.', action='store_true')
        parser.add_argument('--dry-run', help='', action='store_true')
        args = parser.parse_args()

        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s | %(message)s',
            datefmt='%y-%m-%d %H:%M:%S')
        logging.debug('numpy version: %s' % (np.__version__,))
        logging.debug('pandas version: %s' % (pd.__version__,))
        logging.debug('pyBigWig.numpy flag: %s' % (pyBigWig.numpy))
        logging.debug('IDR version: %s' % (yapc.idr.idr.__version__))

        if len(args.CONDITION_REP1_REP2) == 1: # Parse input args as specifying a separate sample sheet
            df_samples = parse_input_table(args.CONDITION_REP1_REP2[0], args.condition_col, args.replicate_col, args.track_col)

        else: # Parse input args as a list of tokens specifying the samples
            df_samples = parse_input_files(args.CONDITION_REP1_REP2, args.pseudoreplicates)

        s_ = df_samples.to_string(line_width=1000, index=False)
        logging.info('Input samples:\n%(s_)s\n' % locals())

        logging.info('Output prefix: %s' % (args.OUTPUT_PREFIX,))
        # Make output directory + intermediate directories if necessary -- fails sometimes
        #prefix_out = sys.argv[-1]
        #makedirsp(os.path.split(prefix_out)[0])

        if args.dry_run: sys.exit(0)

        yapc.peak_calling.main(
            df_samples=df_samples,
            prefix_out=args.OUTPUT_PREFIX,
            kernel=yapc.peak_calling.prepare_second_derivative_kernel(args.smoothing_window_width, args.smoothing_times),
            min_concave_region_width=args.min_concave_region_width,
            fixed_peak_halfwidth=args.fixed_peak_halfwidth,
            truncate_idr_input=args.truncate_idr_input,
            pseudoreplicates=args.pseudoreplicates,
            recycle=args.recycle,
            chroms_subset=str(args.chroms_subset).split(',') if not(args.chroms_subset is None) else None
        )

    except KeyboardInterrupt:
        #logging.warning('Interrupted')
        sys.exit(1)
